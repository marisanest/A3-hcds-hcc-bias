{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Course Human-Centered Data Science ([HCDS](https://www.mi.fu-berlin.de/en/inf/groups/hcc/teaching/winter_term_2020_21/course_human_centered_data_science.html)) - Winter Term 2020/21 - [HCC](https://www.mi.fu-berlin.de/en/inf/groups/hcc/index.html) | [Freie Universität Berlin](https://www.fu-berlin.de/)\n",
    "\n",
    "***\n",
    "\n",
    "# A2 - Wikipedia, ORES, and Bias in Data\n",
    "Please follow the reproducability workflow as practiced during the last exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1⃣ | Data acquisition\n",
    "\n",
    "You will use two data sources: (1) Wikipedia articles of politicians and (2) world population data.\n",
    "\n",
    "**Wikipedia articles -**\n",
    "The Wikipedia articles can be found on [Figshare](https://figshare.com/articles/Untitled_Item/5513449). It contains politiciaans by country from the English-language wikipedia. Please read through the documentation for this repository, then download and unzip it to extract the data file, which is called `page_data.csv`.\n",
    "\n",
    "**Population data -**\n",
    "The population data is available in `CSV` format in the `_data` folder. The file is named `export_2019.csv`. This dataset is drawn from the [world population datasheet](https://www.prb.org/international/indicator/population/table/) published by the Population Reference Bureau (downloaded 2020-11-13 10:14 AM). I have edited the dataset to make it easier to use in this assignment. The population per country is given in millions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of al we import all needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define some helper functions, which are needed for the follwing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_zip_file(url, save_path, chunk_size=128):\n",
    "    '''\n",
    "    Downloads a zip from a given url. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        Url of zip file\n",
    "    save_path : str\n",
    "        Save path of zip file\n",
    "    chunk_size: int\n",
    "        Chuck size in which the zip gile is downloaded \n",
    "    '''\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)\n",
    "            \n",
    "def extract_zip_file(path):\n",
    "    '''\n",
    "    Extracts a zip.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    path : str\n",
    "        Path of zip file\n",
    "    '''\n",
    "    with zipfile.ZipFile(path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(path.replace('.zip', ''))\n",
    "    \n",
    "def move_to_data_raw(dir_path, filename):\n",
    "    '''\n",
    "    Moves a file into the data_raw directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    dir_path : str\n",
    "        Directory path of file\n",
    "    filemame : str\n",
    "        Filename of file\n",
    "    '''\n",
    "    shutil.copyfile(f\"{dir_path}{filename}\", f\"../data_raw/{filename}\")\n",
    "    \n",
    "def remove_zip_files(path):\n",
    "    '''\n",
    "    Removes all zip file dependent files and directories.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    path : str\n",
    "        Path to zip file\n",
    "    '''\n",
    "    os.remove(path)\n",
    "    shutil.rmtree(path.replace('.zip', ''))\n",
    "\n",
    "def save_data_raw(data, filename):\n",
    "    '''\n",
    "    Saves data to the data raw directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    data : dict, pandas.core.frame.DataFrame \n",
    "        Data which should be saved\n",
    "    filename:\n",
    "        Resulting filename\n",
    "    '''\n",
    "    __save(data, '../data_raw/', filename)\n",
    "    \n",
    "def save_data_clean(data, filename):\n",
    "    '''\n",
    "    Saves data to the data clean directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    data : dict, pandas.core.frame.DataFrame\n",
    "        Data which should be saved\n",
    "    filename : str\n",
    "        Resulting filename\n",
    "    '''\n",
    "    __save(data, '../data_clean/', filename)\n",
    "    \n",
    "def save_results(data, filename):\n",
    "    '''\n",
    "    Saves data to the results directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    data : dict, pandas.core.frame.DataFrame\n",
    "        Data which should be saved\n",
    "    filename : str\n",
    "        Resulting filename\n",
    "    '''\n",
    "    __save(data, '../results/', filename)\n",
    "    \n",
    "def __save(data, path, filename):\n",
    "    '''\n",
    "    Saves data to a given directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    data : dict, pandas.core.frame.DataFrame\n",
    "        Data which should be saved\n",
    "    path : str\n",
    "        Directory path\n",
    "    filename:\n",
    "        Resulting filename\n",
    "    '''\n",
    "    data.to_csv(f'{path}{filename}', index=False)\n",
    "    \n",
    "def read_data_raw(filename, sep=','):\n",
    "    '''\n",
    "    Reads data from the data raw directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Filename of file which should be read\n",
    "    sep : str\n",
    "        Separation character of file\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.core.frame.DataFrame\n",
    "        Read data\n",
    "    '''\n",
    "    return __read('../data_raw/', filename, sep)\n",
    "    \n",
    "def read_data_clean(filename, sep=','):\n",
    "    '''\n",
    "    Reads data from the data clean directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Filename of file which should be read\n",
    "    sep : str\n",
    "        Separation character of file\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.core.frame.DataFrame\n",
    "        Read data\n",
    "    '''\n",
    "    return __read('../data_clean/', filename, sep)\n",
    "\n",
    "def read_results(filename, sep=','):\n",
    "    '''\n",
    "    Reads data from the results directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Filename of file which should be read\n",
    "    sep : str\n",
    "        Separation character of file\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.core.frame.DataFrame\n",
    "        Read data\n",
    "    '''\n",
    "    return __read('../results/', filename, sep)\n",
    "\n",
    "def __read(path, filename, sep=','):\n",
    "    '''\n",
    "    Reads data from a given directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Directory path\n",
    "    filename : str\n",
    "        Filename of file which should be read\n",
    "    sep : str\n",
    "        Separation character of file\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.core.frame.DataFrame\n",
    "        Read data\n",
    "    '''\n",
    "    return pd.read_csv(f'{path}{filename}', sep=sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we download and extract the **Wikipedia articles** zip file. The **Population data** can allready be found in the folder: `_data/export_2019.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ndownloader.figshare.com/files/9614893'\n",
    "zip_path = '_data/country.zip'\n",
    "\n",
    "# download\n",
    "download_zip_file(url, zip_path)\n",
    "#extract\n",
    "extract_zip_file(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The needed files for the analysis `_data/country/country/data/page_data.csv` and `_data/country.zip` are moved to the data raw folder so that we have a starting point in terms of data. All not needed zip file related data is removed afterwards to keep the directories clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move\n",
    "move_to_data_raw('_data/country/country/data/', 'page_data.csv')\n",
    "move_to_data_raw('_data/', 'export_2019.csv')\n",
    "\n",
    "# remove\n",
    "remove_zip_files(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2⃣ | Data processing and cleaning\n",
    "The data in `page_data.csv` contain some rows that you will need to filter out. It contains some page names that start with the string `\"Template:\"`. These pages are not Wikipedia articles, and should not be included in your analysis. The data in `export_2019.csv` does not need any cleaning.\n",
    "\n",
    "***\n",
    "\n",
    "| | `page_data.csv` | | |\n",
    "|-|------|---------|--------|\n",
    "| | **page** | **country** | **rev_id** |\n",
    "|0|\tTemplate:ZambiaProvincialMinisters | Zambia | 235107991 |\n",
    "|1|\tBir I of Kanem | Chad | 355319463 |\n",
    "\n",
    "***\n",
    "\n",
    "| | `export_2019.csv` | | |\n",
    "|-|------|---------|--------|\n",
    "| | **country** | **population** | **region** |\n",
    "|0|\tAlgeria | 44.357 | AFRICA |\n",
    "|1|\tEgypt | 100.803 | 355319463 |\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process the data we first load both files from the `data_raw` folder as data frames. From the page data we then remove all entries that are templates and thus not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "page_data = read_data_raw('page_data.csv')\n",
    "country_data = read_data_raw('export_2019.csv', ';')\n",
    "\n",
    "# remove template pages\n",
    "page_data = page_data[~page_data['page'].str.contains('Template:')] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting article quality predictions with ORES\n",
    "\n",
    "Now you need to get the predicted quality scores for each article in the Wikipedia dataset. We're using a machine learning system called [**ORES**](https://www.mediawiki.org/wiki/ORES) (\"Objective Revision Evaluation Service\"). ORES estimates the quality of an article (at a particular point in time), and assigns a series of probabilities that the article is in one of the six quality categories. The options are, from best to worst:\n",
    "\n",
    "| ID | Quality Category |  Explanation |\n",
    "|----|------------------|----------|\n",
    "| 1 | FA    | Featured article |\n",
    "| 2 | GA    | Good article |\n",
    "| 3 | B     | B-class article |\n",
    "| 4 | C     | C-class article |\n",
    "| 5 | Start | Start-class article |\n",
    "| 6 | Stub  | Stub-class article |\n",
    "\n",
    "For context, these quality classes are a sub-set of quality assessment categories developed by Wikipedia editors. If you're curious, you can [read more](https://en.wikipedia.org/wiki/Wikipedia:Content_assessment#Grades) about what these assessment classes mean on English Wikipedia. For this assignment, you only need to know that these categories exist, and that ORES will assign one of these six categories to any `rev_id`. You need to extract all `rev_id`s in the `page_data.csv` file and use the ORES API to get the predicted quality score for that specific article revision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORES REST API endpoint\n",
    "\n",
    "The [ORES REST API](https://ores.wikimedia.org/v3/#!/scoring/get_v3_scores_context_revid_model) is configured fairly similarly to the pageviews API we used for the last assignment. It expects the following parameters:\n",
    "\n",
    "* **project** --> `enwiki`\n",
    "* **revid** --> e.g. `235107991` or multiple ids e.g.: `235107991|355319463` (batch)\n",
    "* **model** --> `wp10` - The name of a model to use when scoring.\n",
    "\n",
    "**❗Note on batch processing:** Please read the documentation about [API usage](https://www.mediawiki.org/wiki/ORES#API_usage) if you want to query a large number of revisions (batches). \n",
    "\n",
    "You will notice that ORES returns a prediction value that contains the name of one category (e.g. `Start`), as well as probability values for each of the six quality categories. For this assignment, you only need to capture and use the value for prediction.\n",
    "\n",
    "**❗Note:** It's possible that you will be unable to get a score for a particular article. If that happens, make sure to maintain a log of articles for which you were not able to retrieve an ORES score. This log should be saved as a separate file named `ORES_no_scores.csv` and should include the `page`, `country`, and `rev_id` (just as in `page_data.csv`).\n",
    "\n",
    "You can use the following **sample code for API calls**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define function and header infromation to fetch data from the **ORES REST API** and process ths fetched data so that we get quality predictions for each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header information for the ORES API call\n",
    "headers = {\n",
    "    'User-Agent': 'https://github.com/marisanest',\n",
    "    'From': 'marisa.f.nest@fu-berlin.de'\n",
    "}\n",
    "\n",
    "def get_ores_data(rev_ids, headers):\n",
    "    '''\n",
    "    Fetches ORES scores for given rev ids. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rev_ids : list\n",
    "        List of rev ids.\n",
    "    headers : dict\n",
    "        headers for ORES call\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        ORES scores as dict\n",
    "    '''\n",
    "    \n",
    "    # Define the endpoint\n",
    "    # https://ores.wikimedia.org/scores/enwiki/?models=wp10&revids=807420979|807422778\n",
    "    endpoint = 'https://ores.wikimedia.org/v3/scores/{project}/?models={model}&revids={revids}'\n",
    "\n",
    "    # Convert the rev ids to valid parameters\n",
    "    if len(rev_ids) > 1:\n",
    "        rev_ids = '|'.join(str(rev_id) for rev_id in rev_ids)\n",
    "    else:\n",
    "        rev_ids = str(rev_ids[0])\n",
    "    \n",
    "    # Define parameters\n",
    "    params = {\n",
    "        'project' : 'enwiki',\n",
    "        'model'   : 'wp10',\n",
    "        'revids'  : rev_ids\n",
    "    }\n",
    "    \n",
    "    # Call API \n",
    "    api_call = requests.get(endpoint.format(**params))\n",
    "    # Covert response to a dict\n",
    "    response = api_call.json()\n",
    "    \n",
    "    return response\n",
    "\n",
    "def get_batches(data, batch_size=50):\n",
    "    '''\n",
    "    Converts given data into batches with given size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.core.frame.DataFrame\n",
    "        Data which sould be converted to batches\n",
    "    batch_size : int\n",
    "        Size of each batch\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.core.groupby.generic.DataFrameGroupBy\n",
    "        Batches\n",
    "    '''\n",
    "    return data.groupby(np.arange(len(data)) // batch_size)\n",
    "\n",
    "def get_quality_scores(page_data):\n",
    "    '''\n",
    "    Fetches ORES quality scores for given pages.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    page_data : pandas.core.frame.DataFrame\n",
    "        Pages for which quality scores should be fetched.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Valids quality scores for each page\n",
    "    dict\n",
    "        Pages for which no valid quality scores could be fetched\n",
    "    '''\n",
    "    \n",
    "    # Init the quality score data\n",
    "    quality_score_data = pd.DataFrame(\n",
    "        {\n",
    "            'page': [], \n",
    "            'country': [], \n",
    "            'rev_id': [], \n",
    "            'quality_score': []\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Init the error data\n",
    "    error_data = pd.DataFrame(\n",
    "        {\n",
    "            'page': [], \n",
    "            'country': [], \n",
    "            'rev_id': []\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Get page data as batches with batch_size = 50\n",
    "    batches = get_batches(page_data)\n",
    "\n",
    "    # Iterate overall all batches\n",
    "    for index, batch in (batches):\n",
    "        print(f'Batch {index + 1}/{len(batches)}', end='\\r')\n",
    "        \n",
    "        # Extract all batch rev ids as list\n",
    "        batch_rev_ids = batch.rev_id.values\n",
    "        # Convert rev id list to \n",
    "        \n",
    "        # Get ORES data for all batch rev ids\n",
    "        ores_data = get_ores_data(batch_rev_ids, headers)\n",
    "    \n",
    "        # Iterate overall all pages within the batch\n",
    "        for index, page in batch.iterrows():\n",
    "            try:\n",
    "                # Extract quality scores from ORES data\n",
    "                quality_score = ores_data['enwiki']['scores'][str(page.rev_id)]['wp10']['score']['prediction']\n",
    "                \n",
    "                # If no error occured (KeyError due to missing quality score), append quality score to quality score data\n",
    "                quality_score_data = quality_score_data.append(\n",
    "                    {\n",
    "                        **dict(page), \n",
    "                        **{'quality_score': quality_score}\n",
    "                    }, \n",
    "                    ignore_index=True\n",
    "                ) \n",
    "            except KeyError:\n",
    "                # If error occured (KeyError due to missing quality score), append error to error data\n",
    "                error_data = error_data.append(dict(page), ignore_index=True)\n",
    "\n",
    "    # Convert rev id columns to int\n",
    "    quality_score_data.rev_id = quality_score_data.rev_id.astype(int)\n",
    "    error_data.rev_id = error_data.rev_id.astype(int)\n",
    "    \n",
    "    # Return data\n",
    "    return quality_score_data, error_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sending one request for each `rev_id` might take some time. If you want to send batches you can use `'|'.join(str(x) for x in revision_ids` to put your ids together. Please make sure to deal with [exception handling](https://www.w3schools.com/python/python_try_except.asp) of the `KeyError` exception, when extracting the `prediction` from the `JSON` response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the datasets\n",
    "\n",
    "Now you need to combine both dataset: (1) the wikipedia articles and its ORES quality scores and (2) the population data. Both have columns named `country`. After merging the data, you'll invariably run into entries which cannot be merged. Either the population dataset does not have an entry for the equivalent Wikipedia country, or vis versa.\n",
    "\n",
    "Please remove any rows that do not have matching data, and output them to a `CSV` file called `countries_no_match.csv`. Consolidate the remaining data into a single `CSV` file called `politicians_by_country.csv`.\n",
    "\n",
    "The schema for that file should look like the following table:\n",
    "\n",
    "\n",
    "| article_name | country | region | revision_id | article_quality | population |\n",
    "|--------------|---------|--------|-------------|-----------------|------------|\n",
    "| Bir I of Kanem | Chad  | AFRICA | 807422778 | Stub | 16877000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we fetch ORES quality scores for each page. All pages for which no quality scores could be found are saved within the `data_clean` directory to keep trak of all occuring errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get quality score data\n",
    "quality_score_data, quality_score_error_data = get_quality_score_data(page_data)\n",
    "\n",
    "# Save error data\n",
    "save_data_clean(quality_score_error_data, 'ORES_no_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fetched quality scores (including all page info) and the country data are then merged, processed and saved to the `data_clean` directory to get a final processed data file for the following analyis step. Again error data (pages for which no matching country could be found) is saved as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge quality score data and country data by country\n",
    "page_country_data = pd.merge(prediction_data, country_data, on=['country'], how='outer')\n",
    "\n",
    "# Extract error data from merged data (all rows with at least one value = NaN)\n",
    "page_country_error_data = page_country_data[page_country_data.isnull().any(axis=1)]\n",
    "\n",
    "# Save error data\n",
    "save_data_clean(page_country_error_data, 'countries_no_match.csv')\n",
    "\n",
    "# Extract valid data from merged data (all rows without any value = NaN)\n",
    "page_country_data = page_country_data[~page_country_data.isnull().any(axis=1)]\n",
    "\n",
    "# Save merged data\n",
    "save_data_clean(page_country_data, 'politicians_by_country.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3⃣ | Analysis\n",
    "\n",
    "Your analysis will consist of calculating the proportion (as a percentage) of articles-per-population (we can also call it `coverage`) and high-quality articles (we can also call it `relative-quality`)for **each country** and for **each region**. By `\"high quality\"` arcticle we mean an article that ORES predicted as `FA` (featured article) or `GA` (good article).\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "* if a country has a population of `10,000` people, and you found `10` articles about politicians from that country, then the percentage of `articles-per-population` would be `0.1%`.\n",
    "* if a country has `10` articles about politicians, and `2` of them are `FA` or `GA` class articles, then the percentage of `high-quality-articles` would be `20%`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results format\n",
    "\n",
    "The results from this analysis are six `data tables`. Embed these tables in the Jupyter notebook. You do not need to graph or otherwise visualize the data for this assignment. The tables will show:\n",
    "\n",
    "1. **Top 10 countries by coverage**<br>10 highest-ranked countries in terms of number of politician articles as a proportion of country population\n",
    "1. **Bottom 10 countries by coverage**<br>10 lowest-ranked countries in terms of number of politician articles as a proportion of country population\n",
    "1. **Top 10 countries by relative quality**<br>10 highest-ranked countries in terms of the relative proportion of politician articles that are of GA and FA-quality\n",
    "1. **Bottom 10 countries by relative quality**<br>10 lowest-ranked countries in terms of the relative proportion of politician articles that are of GA and FA-quality\n",
    "1. **Regions by coverage**<br>Ranking of regions (in descending order) in terms of the total count of politician articles from countries in each region as a proportion of total regional population\n",
    "1. **Regions by relative quality**<br>Ranking of regions (in descending order) in terms of the relative proportion of politician articles from countries in each region that are of GA and FA-quality\n",
    "\n",
    "**❗Hint:** You will find what country belongs to which region (e.g. `ASIA`) also in `export_2019.csv`. You need to calculate the total poulation per region. For that you could use `groupby` and also check out `apply`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the analysis, we load the resulting data from step 2 as data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politician_country_data = read_data_clean('politicians_by_country.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we do all coverage dependent analysis. Therefore we define a helper function which prepares the data for the desired tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coverage_data(data, groupby, agg):\n",
    "    '''\n",
    "    Generates coverage data for given groupby and aggregation parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.core.frame.DataFrame\n",
    "        Data for which coverage data should be generated\n",
    "    groupby : list\n",
    "        Column names which should be grouped\n",
    "    agg : dict\n",
    "        Parameters which define the aggregation method\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.core.frame.DataFrame\n",
    "        Coverage data\n",
    "    '''\n",
    "    # Group\n",
    "    coverage_data = data.groupby(groupby)\n",
    "    # Aggregate\n",
    "    coverage_data = coverage_data.agg(**agg)\n",
    "    # Reset index\n",
    "    coverage_data = coverage_data.reset_index()\n",
    "    # Calculate coverage\n",
    "    coverage_data['coverage'] = coverage_data.politicians / (coverage_data.population * 1e6)\n",
    "    # Sort coverage descending\n",
    "    coverage_data = coverage_data.sort_values('coverage', ascending=False)\n",
    "    # Drop not needed columns\n",
    "    coverage_data = coverage_data.drop(columns=['population', 'politicians'])\n",
    "    # Reset index and drop old index\n",
    "    coverage_data = coverage_data.reset_index(drop=True)\n",
    "    \n",
    "    return coverage_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we generate the needed data for the coverage dependent analysis: once per country and once per region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country dependent coverage data\n",
    "country_coverage_data = get_coverage_data(\n",
    "    politician_country_data, \n",
    "    ['country', 'population'], \n",
    "    {\n",
    "        'politicians':('population', 'count')\n",
    "    }\n",
    ")\n",
    "\n",
    "# Region dependent coverage data\n",
    "region_coverage_data = get_coverage_data(\n",
    "    politician_country_data, \n",
    "    ['region'], \n",
    "    {\n",
    "        'population':('population', 'sum'), \n",
    "        'politicians':('population', 'count')\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each output table is saved to the `results` directory and afterward loaded and shown within the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Top 10 countries by coverage**<br>10 highest-ranked countries in terms of number of politician articles as a proportion of country population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(country_coverage_data.head(10), 'country_coverage_data_top_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_results('country_coverage_data_top_10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Bottom 10 countries by coverage**<br>10 lowest-ranked countries in terms of number of politician articles as a proportion of country population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(country_coverage_data.tail(10), 'country_coverage_data_bottom_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_results('country_coverage_data_bottom_10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Regions by coverage**<br>Ranking of regions (in descending order) in terms of the total count of politician articles from countries in each region as a proportion of total regional population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(region_coverage_data, 'region_coverage_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_results('region_coverage_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second we do all relative quality dependent analysis. Therefore we define a helper functions which prepares the data for the desired tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolut_quality_count(pages):\n",
    "    '''\n",
    "    Counts all pages with a high quality.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pages : pandas.core.frame.DataFrame\n",
    "        Pages for which high quality should be counted\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Counter\n",
    "    '''\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for quality in pages.values:\n",
    "        if quality in ['GA', 'FA']:\n",
    "            counter += 1;\n",
    "            \n",
    "    return counter;\n",
    "\n",
    "def get_relative_quality_data(data, groupby, agg):\n",
    "    '''\n",
    "    Generates relative quality data for given groupby and aggregation parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.core.frame.DataFrame\n",
    "        Data for which relative quality data should be generated\n",
    "    groupby : list\n",
    "        Column names which should be grouped\n",
    "    agg : dict\n",
    "        Parameters which define the aggregation method\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.core.frame.DataFrame\n",
    "        Relative quality data\n",
    "    '''\n",
    "    # Group\n",
    "    relative_quality_data = data.groupby(groupby)\n",
    "    # Aggregate\n",
    "    relative_quality_data = relative_quality_data.agg(**agg)\n",
    "    # Reset index\n",
    "    relative_quality_data = relative_quality_data.reset_index()\n",
    "    # Calculate relative quality\n",
    "    relative_quality_data['relative_quality'] = relative_quality_data.absolut_quality / relative_quality_data.politicians\n",
    "    # Sort relative quality descending\n",
    "    relative_quality_data = relative_quality_data.sort_values('relative_quality', ascending=False)\n",
    "    # Drop not needed columns\n",
    "    relative_quality_data = relative_quality_data.drop(columns=['politicians', 'absolut_quality'])\n",
    "    # Reset index and drop old index\n",
    "    relative_quality_data = relative_quality_data.reset_index(drop=True)\n",
    "    \n",
    "    return relative_quality_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we generate the needed data for the relative quality dependent analysis: once per country and once per region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country dependent Relative quality data\n",
    "country_relative_quality_data = get_relative_quality_data(\n",
    "    politician_country_data, \n",
    "    ['country'], \n",
    "    {\n",
    "        'politicians': ('prediction', 'count'), \n",
    "        'absolut_quality': ('prediction', absolut_quality_count)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Region dependent Relative quality data\n",
    "region_relative_quality_data = get_relative_quality_data(\n",
    "    politician_country_data, \n",
    "    ['region'], \n",
    "    {\n",
    "        'politicians': ('prediction', 'count'), \n",
    "        'absolut_quality': ('prediction', absolut_quality_count)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each output table is saved to the `results` directory and afterward loaded and shown within the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Top 10 countries by relative quality**<br>10 highest-ranked countries in terms of the relative proportion of politician articles that are of GA and FA-quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(country_relative_quality_data.head(10), 'country_relative_quality_data_top_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_results('country_relative_quality_data_top_10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Bottom 10 countries by relative quality**<br>10 lowest-ranked countries in terms of the relative proportion of politician articles that are of GA and FA-quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(country_relative_quality_data.tail(10), 'country_relative_quality_data_bottom_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_results('country_relative_quality_data_bottom_10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Regions by relative quality**<br>Ranking of regions (in descending order) in terms of the relative proportion of politician articles from countries in each region that are of GA and FA-quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(region_relative_quality_data, 'region_relative_quality_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_results('region_relative_quality_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### Credits\n",
    "\n",
    "This exercise is slighty adapted from the course [Human Centered Data Science (Fall 2019)](https://wiki.communitydata.science/Human_Centered_Data_Science_(Fall_2019)) of [Univeristy of Washington](https://www.washington.edu/datasciencemasters/) by [Jonathan T. Morgan](https://wiki.communitydata.science/User:Jtmorgan).\n",
    "\n",
    "Same as the original inventors, we release the notebooks under the [Creative Commons Attribution license (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
